{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52142ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytescode/My_Skill/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70390b06",
   "metadata": {},
   "outputs": [
    {
     "ename": "SSLError",
     "evalue": "(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/mistralai/Mistral-7B-Instruct-v0.2?expand=inferenceProviderMapping (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1032)')))\"), '(Request ID: b1d0868a-6e5e-438b-ba4f-cb84c1f1ddff)')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSSLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[31mSSLError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1032)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/urllib3/util/retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/mistralai/Mistral-7B-Instruct-v0.2?expand=inferenceProviderMapping (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1032)')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mSSLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m model = ChatHuggingFace(llm=llm)\n\u001b[32m      9\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mExplain LangChain in simple terms. In 7 bullet points.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1225\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1223\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1229\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/langchain_huggingface/chat_models/huggingface.py:750\u001b[39m, in \u001b[36mChatHuggingFace._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m     message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    744\u001b[39m     params = {\n\u001b[32m    745\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m    746\u001b[39m         **params,\n\u001b[32m    747\u001b[39m         **({\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[32m    748\u001b[39m         **kwargs,\n\u001b[32m    749\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m     answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(answer)\n\u001b[32m    752\u001b[39m llm_input = \u001b[38;5;28mself\u001b[39m._to_chat_prompt(messages)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:878\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    875\u001b[39m payload_model = model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# Get the provider helper\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m provider_helper = \u001b[43mget_provider_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversational\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpayload_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[38;5;66;03m# Prepare the payload\u001b[39;00m\n\u001b[32m    887\u001b[39m parameters = {\n\u001b[32m    888\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    889\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    906\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    907\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/huggingface_hub/inference/_providers/__init__.py:216\u001b[39m, in \u001b[36mget_provider_helper\u001b[39m\u001b[34m(provider, task, model)\u001b[39m\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSpecifying a model is required when provider is \u001b[39m\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     provider_mapping = \u001b[43m_fetch_inference_provider_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     provider = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(provider_mapping)).provider\n\u001b[32m    219\u001b[39m provider_tasks = PROVIDERS.get(provider)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/huggingface_hub/inference/_providers/_common.py:307\u001b[39m, in \u001b[36m_fetch_inference_provider_mapping\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03mFetch provider mappings for a model from the Hub.\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhf_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m info = \u001b[43mHfApi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minferenceProviderMapping\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m provider_mapping = info.inference_provider_mapping\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/huggingface_hub/hf_api.py:2660\u001b[39m, in \u001b[36mHfApi.model_info\u001b[39m\u001b[34m(self, repo_id, revision, timeout, securityStatus, files_metadata, expand, token)\u001b[39m\n\u001b[32m   2658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m expand:\n\u001b[32m   2659\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mexpand\u001b[39m\u001b[33m\"\u001b[39m] = expand\n\u001b[32m-> \u001b[39m\u001b[32m2660\u001b[39m r = \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2661\u001b[39m hf_raise_for_status(r)\n\u001b[32m   2662\u001b[39m data = r.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:95\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     97\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My_Skill/.venv/lib/python3.13/site-packages/requests/adapters.py:675\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    671\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request=request)\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    674\u001b[39m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m    677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mSSLError\u001b[39m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/mistralai/Mistral-7B-Instruct-v0.2?expand=inferenceProviderMapping (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1032)')))\"), '(Request ID: b1d0868a-6e5e-438b-ba4f-cb84c1f1ddff)')"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3e52836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a Python library for building conversational AI systems. Here's an overview of LangChain in simple terms:\n",
      "\n",
      "‚Ä¢ **Conversational AI**: LangChain helps developers create conversational AI systems like chatbots, virtual assistants, or voice assistants that can have human-like conversations.\n",
      "‚Ä¢ **Modular Architecture**: LangChain uses a modular architecture, which means you can easily integrate different components (like text generation, reasoning, or retrieval) to build complex conversational systems.\n",
      "‚Ä¢ **LlamaIndex**: LangChain has a built-in database called LlamaIndex, which allows you to store and retrieve knowledge. Think of it like a giant library of text data that your conversational AI can draw from.\n",
      "‚Ä¢ **Reasoning and Inference**: LangChain enables AI systems to reason and make inferences based on the knowledge stored in LlamaIndex. This means your chatbot can use the data to make decisions or respond to complex questions.\n",
      "‚Ä¢ **Natural Language Generation**: LangChain provides tools for generating human-like text, making it easier to create conversational responses that sound natural and engaging.\n",
      "‚Ä¢ **Flexibility and Customization**: LangChain is highly customizable, allowing you to tailor the conversational AI system to specific use cases or domains. You can also integrate external models or libraries\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14679c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **What it is:** LangChain is a software framework that lets developers build applications that talk to large language models (like GPT‚Äë4) in a structured, reusable way.  \n",
      "- **Why it matters:** Instead of writing raw API calls, you can chain together small ‚Äúcomponents‚Äù (prompt templates, memory, tools, etc.) to create more complex, reliable workflows.  \n",
      "- **Core idea ‚Äì ‚Äúchain‚Äù:** Think of a chain as a pipeline: data flows from one step to the next (e.g., fetch data ‚Üí ask a question ‚Üí interpret the answer).  \n",
      "- **Built‚Äëin tools:** It ships with ready‚Äëmade connectors for databases, APIs, web scraping, and even external software (e.g., calling a spreadsheet).  \n",
      "- **Memory management:** LangChain can keep track of past interactions, so the model ‚Äúremembers‚Äù context across turns without you having to store it manually.  \n",
      "- **Extensibility:** You can plug in your own custom logic, models, or services, and the framework will handle the orchestration for you.  \n",
      "- **Use cases:** From chatbots and virtual assistants to data‚Äëanalysis helpers, code generators, and automated report writers, LangChain turns language‚Äëmodel power into practical tools.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"openai/gpt-oss-20b\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "577c615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **What it is:** LangChain is a Python (and JavaScript) library that helps you build applications powered by large language models (LLMs) like GPT‚Äë4.  \n",
      "- **Why it exists:** Using LLMs directly is easy for a single prompt, but real‚Äëworld apps need things like memory, tool use, and dynamic prompting‚ÄîLangChain provides the building blocks for those complexities.  \n",
      "- **Core idea ‚Äì ‚ÄúChains‚Äù:** A *chain* is a sequence of steps (prompt ‚Üí LLM ‚Üí post‚Äëprocessing ‚Üí next step). You can stitch together simple components to create sophisticated workflows.  \n",
      "- **Key components:**  \n",
      "  1. **Prompts & Prompt Templates** ‚Äì reusable, parameterized text.  \n",
      "  2. **LLM wrappers** ‚Äì uniform interface to OpenAI, Anthropic, HuggingFace, etc.  \n",
      "  3. **Memory** ‚Äì store conversation history or other context across calls.  \n",
      "  4. **Agents** ‚Äì LLMs that can decide which tool (search, database, calculator) to call next.  \n",
      "  5. **Indexes / Vectorstores** ‚Äì retrieve relevant documents from large corpora for retrieval‚Äëaugmented generation (RAG).  \n",
      "- **Typical workflow:** User query ‚Üí retrieve relevant docs (vectorstore) ‚Üí format prompt with docs ‚Üí LLM generates answer ‚Üí (optional) store result in memory or trigger a tool.  \n",
      "- **Extensibility:** You can plug in your own LLM APIs, custom tools, or data sources; LangChain just defines the glue code.  \n",
      "- **Result:** With a few lines of code you can build chatbots, Q&A assistants, data‚Äëanalysis pipelines, or any app where an LLM needs to interact with external information or maintain context.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"openai/gpt-oss-120b\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527dbe1",
   "metadata": {},
   "source": [
    "# MiMo-V2-Flash\n",
    "#### XiaomiMiMo/MiMo-V2-Flash\n",
    "MiMo-V2-Flash is a Mixture-of-Experts (MoE) language model with **309B** total parameters and 15B active parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f2c5539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an explanation of LangChain in 7 simple bullet points:\n",
      "\n",
      "*   **It is a framework, not an AI:** Think of LangChain as a Lego set or a toolkit. It provides the building blocks to help developers create complex applications using Large Language Models (like GPT-4).\n",
      "*   **It connects LLMs to external data:** Normally, an AI only knows what it was trained on. LangChain allows the AI to read your PDFs, databases, or websites so it can answer questions about your specific data.\n",
      "*   **It remembers conversations:** It gives the AI a \"memory.\" This allows the model to remember what you said five minutes ago, making the conversation feel natural rather than starting over with every new message.\n",
      "*   **It breaks down tasks:** If you ask a complex question, LangChain can prompt the AI to think step-by-step (e.g., \"First, find the data, then calculate the math, then write the summary\") rather than trying to do everything at once.\n",
      "*   **It chains commands together:** As the name suggests, it links different calls to the AI together. It creates a workflow where the output of one step automatically becomes the input for the next.\n",
      "*   **It uses \"Agents\":** These are tools that allow the AI to take actions, such as searching the internet, using a calculator, or querying a specific database to find the correct answer to a prompt.\n",
      "*   **It standardizes coding:** Since every AI model has a slightly different way of being accessed, LangChain acts as a universal translator, allowing developers to swap models without rewriting their entire codebase.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"XiaomiMiMo/MiMo-V2-Flash\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a687751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **A toolbox for AI apps** ‚Äì LangChain is a set of pre-built components and tools that help developers build applications powered by large language models (LLMs) like GPT, without starting from scratch.\n",
      "2. **Connects LLMs to your data** ‚Äì It makes it easy to give an LLM access to your own documents, databases, or APIs, so it can answer questions using your specific information.\n",
      "3. **Manages conversation memory** ‚Äì It can remember the context of a conversation across multiple exchanges, allowing for coherent, multi-turn dialogues (like a chatbot that remembers what you said earlier).\n",
      "4. **Breaks down complex tasks** ‚Äì It can split a big request (like ‚Äúsummarize this 100-page document‚Äù) into smaller, manageable steps that the LLM handles in sequence.\n",
      "5. **Orchestrates chains of actions** ‚Äì The ‚ÄúChain‚Äù in LangChain lets you link multiple steps together‚Äîfor example: fetch data ‚Üí process it with an LLM ‚Üí send the result to another tool.\n",
      "6. **Integrates with many tools** ‚Äì It can connect LLMs to external tools like search engines, calculators, or code executors, making the AI more capable and accurate.\n",
      "7. **Speeds up development** ‚Äì By providing a standard framework, it reduces the repetitive code needed to build, test, and deploy LLM-powered applications.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-V3.2\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd8f6118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "We are going to explain LangChain in simple terms using 7 bullet points.\n",
      " The goal is to make it easy to understand for someone who might not be familiar with the concept.\n",
      " LangChain is a framework for developing applications powered by language models.\n",
      "</think>\n",
      "\n",
      "Here's a simple explanation of LangChain in 7 bullet points:\n",
      "\n",
      "- üß© **Framework for AI Apps**: LangChain is a toolkit for building applications powered by large language models (LLMs) like GPT, simplifying complex AI integrations.\n",
      "\n",
      "- üîó **Chains Components**: It connects different AI tasks (like API calls, data lookups, or calculations) into step-by-step workflows called \"chains\" (e.g., \"fetch data ‚Üí summarize ‚Üí respond\").\n",
      "\n",
      "- üìö **Handles Memory**: Manages conversational context across interactions, allowing AI to remember previous parts of a discussion for coherent, ongoing conversations.\n",
      "\n",
      "- üì¶ **Prebuilt Tools**: Offers ready-made modules for common tasks (text summarization, translation, Q&A) so you don‚Äôt have to build everything from scratch.\n",
      "\n",
      "- üîç **Data Integration**: Easily connects external data sources (PDFs, websites, databases) to let LLMs access and use your specific information.\n",
      "\n",
      "- üß† **Reasoning Capabilities**: Enables LLMs to \"think step-by-step\" (e.g., solve math problems by breaking them into smaller steps) for better accuracy.\n",
      "\n",
      "- ü§ñ **Agent Systems**: Creates \"AI agents\" that can autonomously use tools (like web search or calculators) to answer questions or complete tasks.  \n",
      "\n",
      "Example: A LangChain app could *search the web ‚Üí read a webpage ‚Üí summarize key points ‚Üí translate into Spanish* ‚Äî all in one automated flow.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-R1\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af3c3d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **LangChain is a toolkit** that helps developers build applications using large language models (like ChatGPT or Gemini) in a simple and organized way.\n",
      "\n",
      "- It lets you **connect different tools**‚Äîlike databases, APIs, or search engines‚Äîto a language model so it can answer questions or complete tasks using real-world information.\n",
      "\n",
      "- You can **chain together steps** (like \"find data ‚Üí analyze it ‚Üí give an answer\") so the AI works through a problem step by step, like a smart assistant.\n",
      "\n",
      "- It makes it easier to **build custom AI apps**‚Äîlike chatbots, content generators, or research tools‚Äîwithout having to start from scratch.\n",
      "\n",
      "- LangChain includes **built-in features** like memory, context handling, and error management, so your AI can remember past conversations and stay focused.\n",
      "\n",
      "- It‚Äôs designed for **developers and builders**, not end-users, and supports multiple programming languages like Python.\n",
      "\n",
      "- Think of it as a **\"smart toolbox\"** that lets you plug in AI and real-world tools to build powerful, interactive applications.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f67c5eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- LangChain is a free toolkit that helps you ‚Äúchain‚Äù small pieces of logic (called components) into a longer pipeline so an LLM can do real-world jobs, not just answer one-off questions.  \n",
      "- Think of it as Lego blocks: one block talks to the LLM, another reads your files, another searches the web, another writes to a database, etc. You snap them together in any order you need.  \n",
      "- The most common pattern is: (1) load your private data ‚Üí (2) split it into chunks ‚Üí (3) store the chunks in a searchable ‚Äúvector‚Äù index ‚Üí (4) when the user asks something, find the best chunks ‚Üí (5) feed those chunks plus the question to the LLM ‚Üí (6) return the answer.  \n",
      "- It already has ready-made blocks for things like Gmail, Google Drive, Slack, SQL tables, Python code runners, and more, so you don‚Äôt have to write boiler-plate.  \n",
      "- Memory blocks let the LLM remember what was said earlier in the chat, so the conversation feels continuous instead of starting from scratch each time.  \n",
      "- Chains can call other chains, so you can build big apps (customer-support bot, report generator, code reviewer) by stacking small, testable sub-chains.  \n",
      "- Write everything in plain Python (or JS), run it on your laptop, server, or serverless, and swap LLMs (OpenAI, Hugging-Face, local models) with one-line changes.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"moonshotai/Kimi-K2-Instruct\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a627bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a simple explanation of **LangChain** in 7 bullet points:\n",
      "\n",
      "- **LangChain** is a framework that helps developers build apps using **large language models (LLMs)** like GPT.\n",
      "- It makes it easier to **connect LLMs to external tools and data sources**, like databases or APIs.\n",
      "- It allows you to **chain together steps** ‚Äî for example, asking a question, looking up data, and then giving an answer.\n",
      "- It helps manage **prompt templates**, so you can easily format inputs for the language model.\n",
      "- It supports **memory features**, letting apps remember past interactions (like chat history).\n",
      "- It includes tools for **evaluation**, so you can test and improve how well your app works.\n",
      "- It‚Äôs built for **flexibility and customization**, so developers can use it for many different types of apps, from chatbots to data analysis.\n",
      "\n",
      "Think of LangChain as a **toolkit for building smarter, more powerful apps with AI language models**.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen3-235B-A22B\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ad722a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- LangChain is a free toolkit that helps you plug large-language-model ‚Äúbrains‚Äù (like ChatGPT) into your own data and workflows.  \n",
      "- Think of it as Lego blocks: each block does one job‚Äîread a PDF, call Google, write an answer, store memory, etc.‚Äîand you snap them together.  \n",
      "- It turns plain English questions into repeatable programs (‚Äúchains‚Äù) so the AI can do multi-step tasks, not just one-off replies.  \n",
      "- You can feed it private stuff‚Äîyour company docs, SQL tables, Notion pages‚Äîand it figures out which pieces to grab before answering.  \n",
      "- Memory blocks let the AI remember what you asked 5 minutes ago, so conversations feel continuous.  \n",
      "- It works with Python or JavaScript and hides the messy glue code; you focus on what you want done, not how to prompt the model every time.  \n",
      "- In short, LangChain upgrades the AI from ‚Äúsmart parrot‚Äù to ‚Äúsmart intern‚Äù that can fetch, reason, and act across your tools.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"moonshotai/Kimi-K2-Instruct-0905\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
