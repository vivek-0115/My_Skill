{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f52142ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70390b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. LangChain is a cutting-edge technology that uses artificial intelligence and natural language processing to understand and generate human-like text.\n",
      "2. It is designed to analyze and generate text in various languages, making it a powerful tool for language translation, summarization, and text generation.\n",
      "3. LangChain can be integrated into applications, allowing developers to build language-related features into their software without the need for extensive linguistic expertise.\n",
      "4. It can translate text between different languages with high accuracy, making it useful for global communication and expanding businesses into new markets.\n",
      "5. LangChain can also summarize long texts, making it easier for users to quickly understand complex information.\n",
      "6. It can generate text based on given prompts, allowing for automated content creation and saving time and effort.\n",
      "7. LangChain is constantly learning and improving, making it a versatile and valuable tool for individuals and businesses alike.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3e52836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a Python library for building conversational AI systems. Here's an overview of LangChain in simple terms:\n",
      "\n",
      "‚Ä¢ **Conversational AI**: LangChain helps developers create conversational AI systems like chatbots, virtual assistants, or voice assistants that can have human-like conversations.\n",
      "‚Ä¢ **Modular Architecture**: LangChain uses a modular architecture, which means you can easily integrate different components (like text generation, reasoning, or retrieval) to build complex conversational systems.\n",
      "‚Ä¢ **LlamaIndex**: LangChain has a built-in database called LlamaIndex, which allows you to store and retrieve knowledge. Think of it like a giant library of text data that your conversational AI can draw from.\n",
      "‚Ä¢ **Reasoning and Inference**: LangChain enables AI systems to reason and make inferences based on the knowledge stored in LlamaIndex. This means your chatbot can use the data to make decisions or respond to complex questions.\n",
      "‚Ä¢ **Natural Language Generation**: LangChain provides tools for generating human-like text, making it easier to create conversational responses that sound natural and engaging.\n",
      "‚Ä¢ **Flexibility and Customization**: LangChain is highly customizable, allowing you to tailor the conversational AI system to specific use cases or domains. You can also integrate external models or libraries\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14679c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **What it is:** LangChain is a software framework that lets developers build applications that talk to large language models (like GPT‚Äë4) in a structured, reusable way.  \n",
      "- **Why it matters:** Instead of writing raw API calls, you can chain together small ‚Äúcomponents‚Äù (prompt templates, memory, tools, etc.) to create more complex, reliable workflows.  \n",
      "- **Core idea ‚Äì ‚Äúchain‚Äù:** Think of a chain as a pipeline: data flows from one step to the next (e.g., fetch data ‚Üí ask a question ‚Üí interpret the answer).  \n",
      "- **Built‚Äëin tools:** It ships with ready‚Äëmade connectors for databases, APIs, web scraping, and even external software (e.g., calling a spreadsheet).  \n",
      "- **Memory management:** LangChain can keep track of past interactions, so the model ‚Äúremembers‚Äù context across turns without you having to store it manually.  \n",
      "- **Extensibility:** You can plug in your own custom logic, models, or services, and the framework will handle the orchestration for you.  \n",
      "- **Use cases:** From chatbots and virtual assistants to data‚Äëanalysis helpers, code generators, and automated report writers, LangChain turns language‚Äëmodel power into practical tools.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"openai/gpt-oss-20b\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "577c615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **What it is:** LangChain is a Python (and JavaScript) library that helps you build applications powered by large language models (LLMs) like GPT‚Äë4.  \n",
      "- **Why it exists:** Using LLMs directly is easy for a single prompt, but real‚Äëworld apps need things like memory, tool use, and dynamic prompting‚ÄîLangChain provides the building blocks for those complexities.  \n",
      "- **Core idea ‚Äì ‚ÄúChains‚Äù:** A *chain* is a sequence of steps (prompt ‚Üí LLM ‚Üí post‚Äëprocessing ‚Üí next step). You can stitch together simple components to create sophisticated workflows.  \n",
      "- **Key components:**  \n",
      "  1. **Prompts & Prompt Templates** ‚Äì reusable, parameterized text.  \n",
      "  2. **LLM wrappers** ‚Äì uniform interface to OpenAI, Anthropic, HuggingFace, etc.  \n",
      "  3. **Memory** ‚Äì store conversation history or other context across calls.  \n",
      "  4. **Agents** ‚Äì LLMs that can decide which tool (search, database, calculator) to call next.  \n",
      "  5. **Indexes / Vectorstores** ‚Äì retrieve relevant documents from large corpora for retrieval‚Äëaugmented generation (RAG).  \n",
      "- **Typical workflow:** User query ‚Üí retrieve relevant docs (vectorstore) ‚Üí format prompt with docs ‚Üí LLM generates answer ‚Üí (optional) store result in memory or trigger a tool.  \n",
      "- **Extensibility:** You can plug in your own LLM APIs, custom tools, or data sources; LangChain just defines the glue code.  \n",
      "- **Result:** With a few lines of code you can build chatbots, Q&A assistants, data‚Äëanalysis pipelines, or any app where an LLM needs to interact with external information or maintain context.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"openai/gpt-oss-120b\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527dbe1",
   "metadata": {},
   "source": [
    "# MiMo-V2-Flash\n",
    "#### XiaomiMiMo/MiMo-V2-Flash\n",
    "MiMo-V2-Flash is a Mixture-of-Experts (MoE) language model with **309B** total parameters and 15B active parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f2c5539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an explanation of LangChain in 7 simple bullet points:\n",
      "\n",
      "*   **It is a framework, not an AI:** Think of LangChain as a Lego set or a toolkit. It provides the building blocks to help developers create complex applications using Large Language Models (like GPT-4).\n",
      "*   **It connects LLMs to external data:** Normally, an AI only knows what it was trained on. LangChain allows the AI to read your PDFs, databases, or websites so it can answer questions about your specific data.\n",
      "*   **It remembers conversations:** It gives the AI a \"memory.\" This allows the model to remember what you said five minutes ago, making the conversation feel natural rather than starting over with every new message.\n",
      "*   **It breaks down tasks:** If you ask a complex question, LangChain can prompt the AI to think step-by-step (e.g., \"First, find the data, then calculate the math, then write the summary\") rather than trying to do everything at once.\n",
      "*   **It chains commands together:** As the name suggests, it links different calls to the AI together. It creates a workflow where the output of one step automatically becomes the input for the next.\n",
      "*   **It uses \"Agents\":** These are tools that allow the AI to take actions, such as searching the internet, using a calculator, or querying a specific database to find the correct answer to a prompt.\n",
      "*   **It standardizes coding:** Since every AI model has a slightly different way of being accessed, LangChain acts as a universal translator, allowing developers to swap models without rewriting their entire codebase.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"XiaomiMiMo/MiMo-V2-Flash\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a687751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **A toolbox for AI apps** ‚Äì LangChain is a set of pre-built components and tools that help developers build applications powered by large language models (LLMs) like GPT, without starting from scratch.\n",
      "2. **Connects LLMs to your data** ‚Äì It makes it easy to give an LLM access to your own documents, databases, or APIs, so it can answer questions using your specific information.\n",
      "3. **Manages conversation memory** ‚Äì It can remember the context of a conversation across multiple exchanges, allowing for coherent, multi-turn dialogues (like a chatbot that remembers what you said earlier).\n",
      "4. **Breaks down complex tasks** ‚Äì It can split a big request (like ‚Äúsummarize this 100-page document‚Äù) into smaller, manageable steps that the LLM handles in sequence.\n",
      "5. **Orchestrates chains of actions** ‚Äì The ‚ÄúChain‚Äù in LangChain lets you link multiple steps together‚Äîfor example: fetch data ‚Üí process it with an LLM ‚Üí send the result to another tool.\n",
      "6. **Integrates with many tools** ‚Äì It can connect LLMs to external tools like search engines, calculators, or code executors, making the AI more capable and accurate.\n",
      "7. **Speeds up development** ‚Äì By providing a standard framework, it reduces the repetitive code needed to build, test, and deploy LLM-powered applications.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-V3.2\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd8f6118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "We are going to explain LangChain in simple terms using 7 bullet points.\n",
      " The goal is to make it easy to understand for someone who might not be familiar with the concept.\n",
      " LangChain is a framework for developing applications powered by language models.\n",
      "</think>\n",
      "\n",
      "Here's a simple explanation of LangChain in 7 bullet points:\n",
      "\n",
      "- üß© **Framework for AI Apps**: LangChain is a toolkit for building applications powered by large language models (LLMs) like GPT, simplifying complex AI integrations.\n",
      "\n",
      "- üîó **Chains Components**: It connects different AI tasks (like API calls, data lookups, or calculations) into step-by-step workflows called \"chains\" (e.g., \"fetch data ‚Üí summarize ‚Üí respond\").\n",
      "\n",
      "- üìö **Handles Memory**: Manages conversational context across interactions, allowing AI to remember previous parts of a discussion for coherent, ongoing conversations.\n",
      "\n",
      "- üì¶ **Prebuilt Tools**: Offers ready-made modules for common tasks (text summarization, translation, Q&A) so you don‚Äôt have to build everything from scratch.\n",
      "\n",
      "- üîç **Data Integration**: Easily connects external data sources (PDFs, websites, databases) to let LLMs access and use your specific information.\n",
      "\n",
      "- üß† **Reasoning Capabilities**: Enables LLMs to \"think step-by-step\" (e.g., solve math problems by breaking them into smaller steps) for better accuracy.\n",
      "\n",
      "- ü§ñ **Agent Systems**: Creates \"AI agents\" that can autonomously use tools (like web search or calculators) to answer questions or complete tasks.  \n",
      "\n",
      "Example: A LangChain app could *search the web ‚Üí read a webpage ‚Üí summarize key points ‚Üí translate into Spanish* ‚Äî all in one automated flow.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-R1\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af3c3d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **LangChain is a toolkit** that helps developers build applications using large language models (like ChatGPT or Gemini) in a simple and organized way.\n",
      "\n",
      "- It lets you **connect different tools**‚Äîlike databases, APIs, or search engines‚Äîto a language model so it can answer questions or complete tasks using real-world information.\n",
      "\n",
      "- You can **chain together steps** (like \"find data ‚Üí analyze it ‚Üí give an answer\") so the AI works through a problem step by step, like a smart assistant.\n",
      "\n",
      "- It makes it easier to **build custom AI apps**‚Äîlike chatbots, content generators, or research tools‚Äîwithout having to start from scratch.\n",
      "\n",
      "- LangChain includes **built-in features** like memory, context handling, and error management, so your AI can remember past conversations and stay focused.\n",
      "\n",
      "- It‚Äôs designed for **developers and builders**, not end-users, and supports multiple programming languages like Python.\n",
      "\n",
      "- Think of it as a **\"smart toolbox\"** that lets you plug in AI and real-world tools to build powerful, interactive applications.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f67c5eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- LangChain is a free toolkit that helps you ‚Äúchain‚Äù small pieces of logic (called components) into a longer pipeline so an LLM can do real-world jobs, not just answer one-off questions.  \n",
      "- Think of it as Lego blocks: one block talks to the LLM, another reads your files, another searches the web, another writes to a database, etc. You snap them together in any order you need.  \n",
      "- The most common pattern is: (1) load your private data ‚Üí (2) split it into chunks ‚Üí (3) store the chunks in a searchable ‚Äúvector‚Äù index ‚Üí (4) when the user asks something, find the best chunks ‚Üí (5) feed those chunks plus the question to the LLM ‚Üí (6) return the answer.  \n",
      "- It already has ready-made blocks for things like Gmail, Google Drive, Slack, SQL tables, Python code runners, and more, so you don‚Äôt have to write boiler-plate.  \n",
      "- Memory blocks let the LLM remember what was said earlier in the chat, so the conversation feels continuous instead of starting from scratch each time.  \n",
      "- Chains can call other chains, so you can build big apps (customer-support bot, report generator, code reviewer) by stacking small, testable sub-chains.  \n",
      "- Write everything in plain Python (or JS), run it on your laptop, server, or serverless, and swap LLMs (OpenAI, Hugging-Face, local models) with one-line changes.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"moonshotai/Kimi-K2-Instruct\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a627bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a simple explanation of **LangChain** in 7 bullet points:\n",
      "\n",
      "- **LangChain** is a framework that helps developers build apps using **large language models (LLMs)** like GPT.\n",
      "- It makes it easier to **connect LLMs to external tools and data sources**, like databases or APIs.\n",
      "- It allows you to **chain together steps** ‚Äî for example, asking a question, looking up data, and then giving an answer.\n",
      "- It helps manage **prompt templates**, so you can easily format inputs for the language model.\n",
      "- It supports **memory features**, letting apps remember past interactions (like chat history).\n",
      "- It includes tools for **evaluation**, so you can test and improve how well your app works.\n",
      "- It‚Äôs built for **flexibility and customization**, so developers can use it for many different types of apps, from chatbots to data analysis.\n",
      "\n",
      "Think of LangChain as a **toolkit for building smarter, more powerful apps with AI language models**.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen3-235B-A22B\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ad722a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- LangChain is a free toolkit that helps you plug large-language-model ‚Äúbrains‚Äù (like ChatGPT) into your own data and workflows.  \n",
      "- Think of it as Lego blocks: each block does one job‚Äîread a PDF, call Google, write an answer, store memory, etc.‚Äîand you snap them together.  \n",
      "- It turns plain English questions into repeatable programs (‚Äúchains‚Äù) so the AI can do multi-step tasks, not just one-off replies.  \n",
      "- You can feed it private stuff‚Äîyour company docs, SQL tables, Notion pages‚Äîand it figures out which pieces to grab before answering.  \n",
      "- Memory blocks let the AI remember what you asked 5 minutes ago, so conversations feel continuous.  \n",
      "- It works with Python or JavaScript and hides the messy glue code; you focus on what you want done, not how to prompt the model every time.  \n",
      "- In short, LangChain upgrades the AI from ‚Äúsmart parrot‚Äù to ‚Äúsmart intern‚Äù that can fetch, reason, and act across your tools.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"moonshotai/Kimi-K2-Instruct-0905\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "msg = \"Explain LangChain in simple terms. In 7 bullet points.\"\n",
    "response = model.invoke(msg)\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
